{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 1.7.1+cu110\n",
      "PyTorch Lightning Version: 1.3.8\n"
     ]
    }
   ],
   "source": [
    "debug_mode = True\n",
    "\n",
    "# Switch to training mode from here\n",
    "training_mode = True\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "from pickle import dump, load\n",
    "import glob\n",
    "import re\n",
    "import string\n",
    "import collections\n",
    "import json\n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import Optimizer, Adam, lr_scheduler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "gc.enable()\n",
    "\n",
    "rand_seed = 1120\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"PyTorch Lightning Version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 4\n",
    "gpus = [0]\n",
    "\n",
    "breath_steps = 80\n",
    "\n",
    "epochs = 1000 if not debug_mode else 1\n",
    "\n",
    "train_batch_size = 128\n",
    "infer_batch_size = 2048\n",
    "\n",
    "mixed_precision = False\n",
    "\n",
    "learning_rate = 1e-3\n",
    "learning_rate *= len(gpus)\n",
    "\n",
    "weight_decay = 0\n",
    "\n",
    "# Model hyperparameters #\n",
    "# Explanation reference: https://timeseriesai.github.io/tsai/models.TST.html\n",
    "\n",
    "d_model = 128  # total dimension of the model (number of features created by the model) Usual values: 128-1024.\n",
    "n_heads = 8  # parallel attention heads. Usual values: 8-16.\n",
    "num_layers = 3  # the number of sub-encoder-layers in the encoder. Usual values: 2-8.\n",
    "dim_feedforward = 256  # the dimension of the feedforward network model. Usual values: 256-4096.\n",
    "dropout = 0.1  # amount of residual dropout applied in the encoder. Usual values: 0.-0.3.\n",
    "pos_encoding = 'learnable'  # fixed, learnable\n",
    "activation = 'gelu'  # # activation function of intermediate layer, relu or gelu.\n",
    "norm = 'BatchNorm'  # BatchNorm, LayerNorm\n",
    "\n",
    "lr_decay_steps = 983 * 300  # every K epochs\n",
    "lr_decay_rate = 0.1  # every K epochs\n",
    "\n",
    "mean_mask_length = 3  # Imputation: the desired mean length of masked segments. Used only when `mask_distribution` is 'geometric'.\n",
    "masking_ratio = 0.15  # Imputation: mask this proportion of each variable\n",
    "mask_mode = 'separate'  # Imputation: whether each variable should be masked separately\n",
    "mask_distribution = 'geometric'  # Imputation: whether each mask sequence element is sampled independently at random\n",
    "\n",
    "accumulate_grad_batches = 1\n",
    "gradient_clip_val = 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = f\"lightning-unsupervised-tst\"\n",
    "\n",
    "dataset_folder = f\"./dataset/ventilator-pressure-prediction\"\n",
    "\n",
    "model_output_folder = f\"./{experiment_name}\"\n",
    "os.makedirs(model_output_folder, exist_ok=True)\n",
    "\n",
    "# pretrained_model_path = glob.glob(f'{model_output_folder}/epoch*.ckpt')[0]\n",
    "pretrained_model_path = \"../input/lightning-unsupervised-tst/epoch904-train_loss_epoch0.011572.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6036000, 8), (4024000, 7), (4024000, 2))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(f\"{dataset_folder}/train.csv\")\n",
    "test_df = pd.read_csv(f\"{dataset_folder}/test.csv\")\n",
    "submit_df = pd.read_csv(f\"{dataset_folder}/sample_submission.csv\")\n",
    "train_df.shape, test_df.shape, submit_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breath_id</th>\n",
       "      <th>R</th>\n",
       "      <th>C</th>\n",
       "      <th>time_step</th>\n",
       "      <th>u_in</th>\n",
       "      <th>u_out</th>\n",
       "      <th>pressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>0</td>\n",
       "      <td>5.837492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0.033652</td>\n",
       "      <td>18.383041</td>\n",
       "      <td>0</td>\n",
       "      <td>5.907794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0.067514</td>\n",
       "      <td>22.509278</td>\n",
       "      <td>0</td>\n",
       "      <td>7.876254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0.101542</td>\n",
       "      <td>22.808822</td>\n",
       "      <td>0</td>\n",
       "      <td>11.742872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0.135756</td>\n",
       "      <td>25.355850</td>\n",
       "      <td>0</td>\n",
       "      <td>12.234987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  breath_id   R   C  time_step       u_in  u_out   pressure\n",
       "0   1          1  20  50   0.000000   0.083334      0   5.837492\n",
       "1   2          1  20  50   0.033652  18.383041      0   5.907794\n",
       "2   3          1  20  50   0.067514  22.509278      0   7.876254\n",
       "3   4          1  20  50   0.101542  22.808822      0  11.742872\n",
       "4   5          1  20  50   0.135756  25.355850      0  12.234987"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u_in', 'u_out']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_column = \"pressure\"\n",
    "meta_columns = [\"breath_id\", \"time_step\"]\n",
    "raw_features = [\n",
    "    c for c in train_df.columns\n",
    "    if c not in [\"id\", target_column, \"R\", \"C\"] + meta_columns\n",
    "]\n",
    "raw_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(obj, folder_path):\n",
    "    dump(obj, open(folder_path, 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_pickle(folder_path):\n",
    "    return load(open(folder_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/gzerveas/mvts_transformer/blob/master/src/optimizers.py\n",
    "# From https://github.com/LiyuanLucasLiu/RAdam/blob/master/radam/radam.py\n",
    "class RAdam(Optimizer):\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 lr=1e-3,\n",
    "                 betas=(0.9, 0.999),\n",
    "                 eps=1e-8,\n",
    "                 weight_decay=0,\n",
    "                 degenerated_to_sgd=True):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(\n",
    "                betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(\n",
    "                betas[1]))\n",
    "\n",
    "        self.degenerated_to_sgd = degenerated_to_sgd\n",
    "        if isinstance(params,\n",
    "                      (list, tuple)) and len(params) > 0 and isinstance(\n",
    "                          params[0], dict):\n",
    "            for param in params:\n",
    "                if 'betas' in param and (param['betas'][0] != betas[0]\n",
    "                                         or param['betas'][1] != betas[1]):\n",
    "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
    "        defaults = dict(lr=lr,\n",
    "                        betas=betas,\n",
    "                        eps=eps,\n",
    "                        weight_decay=weight_decay,\n",
    "                        buffer=[[None, None, None] for _ in range(10)])\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\n",
    "                        'RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(\n",
    "                        p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = group['buffer'][int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2**state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 -\n",
    "                                                                       beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = math.sqrt(\n",
    "                            (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) *\n",
    "                            (N_sma - 2) / N_sma * N_sma_max /\n",
    "                            (N_sma_max - 2)) / (1 - beta1**state['step'])\n",
    "                    elif self.degenerated_to_sgd:\n",
    "                        step_size = 1.0 / (1 - beta1**state['step'])\n",
    "                    else:\n",
    "                        step_size = -1\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'],\n",
    "                                         p_data_fp32)\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg,\n",
    "                                         denom)\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "                elif step_size > 0:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'],\n",
    "                                         p_data_fp32)\n",
    "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    df['R'] = df['R'].astype(str)\n",
    "    df['C'] = df['C'].astype(str)\n",
    "    df = pd.get_dummies(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 97.86 Mb (73.4% reduction)\n",
      "Mem. usage decreased to 57.56 Mb (73.2% reduction)\n"
     ]
    }
   ],
   "source": [
    "train_df = reduce_mem_usage(train_df)\n",
    "test_df = reduce_mem_usage(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6036000, 9), (4024000, 9))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features = add_features(train_df[['time_step'] + raw_features +\n",
    "                                       ['R', 'C']].copy())\n",
    "test_features = add_features(test_df[['time_step'] + raw_features +\n",
    "                                     ['R', 'C']].copy())\n",
    "train_features.shape, test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time_step', 'u_in', 'u_out', 'R_20', 'R_5', 'R_50', 'C_10', 'C_20',\n",
       "       'C_50'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = train_df.index.to_numpy().reshape(-1, breath_steps)\n",
    "oof_df = train_df[[\"id\", \"pressure\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "test_features = scaler.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch, steps, features)\n",
    "train_features = train_features.reshape(-1, breath_steps,\n",
    "                                        train_features.shape[-1])\n",
    "test_features = test_features.reshape(-1, breath_steps,\n",
    "                                      test_features.shape[-1])\n",
    "\n",
    "train_u_out = train_df[['u_out']].to_numpy().reshape(-1, breath_steps)\n",
    "test_u_out = test_df[['u_out']].to_numpy().reshape(-1, breath_steps)\n",
    "targets = train_df[['pressure']].to_numpy().reshape(-1, breath_steps)\n",
    "\n",
    "train_breath_ids = train_df[\"breath_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75450, 80, 9), (50300, 80, 9), (75450, 80), (50300, 80), (75450, 80))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape, test_features.shape, train_u_out.shape, test_u_out.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125750, 80, 9) (125750, 80)\n"
     ]
    }
   ],
   "source": [
    "if training_mode:\n",
    "    all_features = np.concatenate([train_features, test_features], axis=0)\n",
    "    all_u_out = np.concatenate([train_u_out, test_u_out], axis=0)\n",
    "    print(all_features.shape, all_u_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "956"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_df, test_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/gzerveas/mvts_transformer\n",
    "\n",
    "def noise_mask(X,\n",
    "               masking_ratio,\n",
    "               lm=3,\n",
    "               mode='separate',\n",
    "               distribution='geometric',\n",
    "               exclude_feats=None):\n",
    "    \"\"\"\n",
    "    Creates a random boolean mask of the same shape as X, with 0s at places where a feature should be masked.\n",
    "    Args:\n",
    "        X: (seq_length, feat_dim) numpy array of features corresponding to a single sample\n",
    "        masking_ratio: proportion of seq_length to be masked. At each time step, will also be the proportion of\n",
    "            feat_dim that will be masked on average\n",
    "        lm: average length of masking subsequences (streaks of 0s). Used only when `distribution` is 'geometric'.\n",
    "        mode: whether each variable should be masked separately ('separate'), or all variables at a certain positions\n",
    "            should be masked concurrently ('concurrent')\n",
    "        distribution: whether each mask sequence element is sampled independently at random, or whether\n",
    "            sampling follows a markov chain (and thus is stateful), resulting in geometric distributions of\n",
    "            masked squences of a desired mean length `lm`\n",
    "        exclude_feats: iterable of indices corresponding to features to be excluded from masking (i.e. to remain all 1s)\n",
    "\n",
    "    Returns:\n",
    "        boolean numpy array with thes same shape as X, with 0s at places where a feature should be maked\n",
    "    \"\"\"\n",
    "    if exclude_feats is not None:\n",
    "        exclude_feats = set(exclude_feats)\n",
    "\n",
    "    if distribution == 'geometric':  # stateful (Markov chain)\n",
    "        if mode == 'separate':  # each variable (feature) is independent\n",
    "            mask = np.ones(X.shape, dtype=bool)\n",
    "            for m in range(X.shape[1]):  # feature dimension\n",
    "                if exclude_feats is None or m not in exclude_feats:\n",
    "                    mask[:, m] = geom_noise_mask_single(\n",
    "                        X.shape[0], lm, masking_ratio)  # time dimension\n",
    "        else:  # replicate across feature dimension (mask all variables at the same positions concurrently)\n",
    "            mask = np.tile(\n",
    "                np.expand_dims(\n",
    "                    geom_noise_mask_single(X.shape[0], lm, masking_ratio), 1),\n",
    "                X.shape[1])\n",
    "    else:  # each position is independent Bernoulli with p = 1 - masking_ratio\n",
    "        if mode == 'separate':\n",
    "            mask = np.random.choice(np.array([True, False]),\n",
    "                                    size=X.shape,\n",
    "                                    replace=True,\n",
    "                                    p=(1 - masking_ratio, masking_ratio))\n",
    "        else:\n",
    "            mask = np.tile(\n",
    "                np.random.choice(np.array([True, False]),\n",
    "                                 size=(X.shape[0], 1),\n",
    "                                 replace=True,\n",
    "                                 p=(1 - masking_ratio, masking_ratio)),\n",
    "                X.shape[1])\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def geom_noise_mask_single(L, lm, masking_ratio):\n",
    "    \"\"\"\n",
    "    Randomly create a boolean mask of length `L`, consisting of subsequences of average length lm, masking with 0s a `masking_ratio`\n",
    "    proportion of the sequence L. The length of masking subsequences and intervals follow a geometric distribution.\n",
    "    Args:\n",
    "        L: length of mask and sequence to be masked\n",
    "        lm: average length of masking subsequences (streaks of 0s)\n",
    "        masking_ratio: proportion of L to be masked\n",
    "\n",
    "    Returns:\n",
    "        (L,) boolean numpy array intended to mask ('drop') with 0s a sequence of length L\n",
    "    \"\"\"\n",
    "    keep_mask = np.ones(L, dtype=bool)\n",
    "    p_m = 1 / lm  # probability of each masking sequence stopping. parameter of geometric distribution.\n",
    "    p_u = p_m * masking_ratio / (\n",
    "        1 - masking_ratio\n",
    "    )  # probability of each unmasked sequence stopping. parameter of geometric distribution.\n",
    "    p = [p_m, p_u]\n",
    "\n",
    "    # Start in state 0 with masking_ratio probability\n",
    "    state = int(np.random.rand() >\n",
    "                masking_ratio)  # state 0 means masking, 1 means not masking\n",
    "    for i in range(L):\n",
    "        keep_mask[\n",
    "            i] = state  # here it happens that state and masking value corresponding to state are identical\n",
    "        if np.random.rand() < p[state]:\n",
    "            state = 1 - state\n",
    "\n",
    "    return keep_mask\n",
    "\n",
    "\n",
    "def padding_mask(lengths, max_len=None):\n",
    "    \"\"\"\n",
    "    Used to mask padded positions: creates a (batch_size, max_len) boolean mask from a tensor of sequence lengths,\n",
    "    where 1 means keep element at this position (time step)\n",
    "    \"\"\"\n",
    "    batch_size = lengths.numel()\n",
    "    max_len = max_len or lengths.max_val(\n",
    "    )  # trick works because of overloading of 'or' operator for non-boolean types\n",
    "    return (torch.arange(0, max_len,\n",
    "                         device=lengths.device).type_as(lengths).repeat(\n",
    "                             batch_size, 1).lt(lengths.unsqueeze(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from: https://github.com/gzerveas/mvts_transformer/blob/master/src/datasets/dataset.py\n",
    "\n",
    "def collate_unsuperv(data, max_len=None, mask_compensation=False):\n",
    "    \"\"\"Build mini-batch tensors from a list of (X, mask) tuples. Mask input. Create\n",
    "    Args:\n",
    "        data: len(batch_size) list of tuples (X, mask).\n",
    "            - X: torch tensor of shape (seq_length, feat_dim); variable seq_length.\n",
    "            - mask: boolean torch tensor of shape (seq_length, feat_dim); variable seq_length.\n",
    "        max_len: global fixed sequence length. Used for architectures requiring fixed length input,\n",
    "            where the batch length cannot vary dynamically. Longer sequences are clipped, shorter are padded with 0s\n",
    "    Returns:\n",
    "        X: (batch_size, padded_length, feat_dim) torch tensor of masked features (input)\n",
    "        targets: (batch_size, padded_length, feat_dim) torch tensor of unmasked features (output)\n",
    "        target_masks: (batch_size, padded_length, feat_dim) boolean torch tensor\n",
    "            0 indicates masked values to be predicted, 1 indicates unaffected/\"active\" feature values\n",
    "        padding_masks: (batch_size, padded_length) boolean tensor, 1 means keep vector at this position, 0 ignore (padding)\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = len(data)\n",
    "    features, masks, u_out = zip(*data)\n",
    "\n",
    "    # Stack and pad features and masks (convert 2D to 3D tensors, i.e. add batch dimension)\n",
    "    lengths = [X.shape[0] for X in features\n",
    "               ]  # original sequence length for each time series\n",
    "    if max_len is None:\n",
    "        max_len = max(lengths)\n",
    "    X = torch.zeros(\n",
    "        batch_size, max_len,\n",
    "        features[0].shape[-1])  # (batch_size, padded_length, feat_dim)\n",
    "    target_masks = torch.zeros_like(\n",
    "        X, dtype=torch.bool\n",
    "    )  # (batch_size, padded_length, feat_dim) masks related to objective\n",
    "    for i in range(batch_size):\n",
    "        end = min(lengths[i], max_len)\n",
    "        X[i, :end, :] = features[i][:end, :]\n",
    "        target_masks[i, :end, :] = masks[i][:end, :]\n",
    "\n",
    "    targets = X.clone()\n",
    "    X = X * target_masks  # mask input\n",
    "    if mask_compensation:\n",
    "        X = compensate_masking(X, target_masks)\n",
    "\n",
    "    padding_masks = torch.zeros(\n",
    "        batch_size, max_len, dtype=torch.bool)  # (batch_size, padded_length)\n",
    "    for i in range(batch_size):\n",
    "        padding_masks[i, :] = torch.where(u_out[i] == 0, 1, 0)\n",
    "\n",
    "    target_masks = ~target_masks  # inverse logic: 0 now means ignore, 1 means predict\n",
    "\n",
    "    return X, targets, target_masks, padding_masks\n",
    "\n",
    "\n",
    "class VPPMaskedInputDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        features,\n",
    "        u_out,\n",
    "        mean_mask_length=3,\n",
    "        masking_ratio=0.15,\n",
    "        mode='separate',\n",
    "        distribution='geometric',\n",
    "    ):\n",
    "        self.features = features\n",
    "        self.u_out = u_out\n",
    "\n",
    "        self.masking_ratio = masking_ratio\n",
    "        self.mean_mask_length = mean_mask_length\n",
    "        self.mode = mode\n",
    "        self.distribution = distribution\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        For a given integer index, returns the corresponding (seq_length, feat_dim) array and a noise mask of same shape\n",
    "        Args:\n",
    "            index: integer index of sample in dataset\n",
    "        Returns:\n",
    "            X: (seq_length, feat_dim) tensor of the multivariate time series corresponding to a sample\n",
    "            mask: (seq_length, feat_dim) boolean tensor: 0s mask and predict, 1s: unaffected input\n",
    "        \"\"\"\n",
    "\n",
    "        X = self.features[index, :, :]  # (seq_length, feat_dim) array\n",
    "\n",
    "        mask = noise_mask(X, self.masking_ratio, self.mean_mask_length,\n",
    "                          self.mode, self.distribution,\n",
    "                          None)  # (seq_length, feat_dim) boolean array\n",
    "\n",
    "        return torch.from_numpy(X), torch.from_numpy(mask), torch.from_numpy(\n",
    "            self.u_out[index, :])\n",
    "\n",
    "    def update(self):\n",
    "        self.mean_mask_length = min(20, self.mean_mask_length + 1)\n",
    "        self.masking_ratio = min(1, self.masking_ratio + 0.05)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPPTestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, u_out):\n",
    "        self.X = data\n",
    "        self.u_out = u_out\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index, :, :], self.u_out[index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMAELoss(nn.Module):\n",
    "    \"\"\" Masked MAE Loss\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction: str = 'mean'):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.reduction = reduction\n",
    "        self.mae_loss = nn.L1Loss(reduction=self.reduction)\n",
    "\n",
    "    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor,\n",
    "                mask: torch.BoolTensor) -> torch.Tensor:\n",
    "        \"\"\"Compute the loss between a target value and a prediction.\n",
    "\n",
    "        Args:\n",
    "            y_pred: Estimated values\n",
    "            y_true: Target values\n",
    "            mask: boolean tensor with 0s at places where values should be ignored and 1s where they should be considered\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        if reduction == 'none':\n",
    "            (num_active,) Loss for each active batch element as a tensor with gradient attached.\n",
    "        if reduction == 'mean':\n",
    "            scalar mean loss over batch as a tensor with gradient attached.\n",
    "        \"\"\"\n",
    "\n",
    "        # for this particular loss, one may also elementwise multiply y_pred and y_true with the inverted mask\n",
    "        masked_pred = torch.masked_select(y_pred, mask)\n",
    "        masked_true = torch.masked_select(y_true, mask)\n",
    "\n",
    "        return self.mae_loss(masked_pred, masked_true)\n",
    "\n",
    "\n",
    "class MaskedMSELoss(nn.Module):\n",
    "    \"\"\" Masked MSE Loss\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction: str = 'mean'):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.reduction = reduction\n",
    "        self.mse_loss = nn.MSELoss(reduction=self.reduction)\n",
    "\n",
    "    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor,\n",
    "                mask: torch.BoolTensor) -> torch.Tensor:\n",
    "        \"\"\"Compute the loss between a target value and a prediction.\n",
    "\n",
    "        Args:\n",
    "            y_pred: Estimated values\n",
    "            y_true: Target values\n",
    "            mask: boolean tensor with 0s at places where values should be ignored and 1s where they should be considered\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        if reduction == 'none':\n",
    "            (num_active,) Loss for each active batch element as a tensor with gradient attached.\n",
    "        if reduction == 'mean':\n",
    "            scalar mean loss over batch as a tensor with gradient attached.\n",
    "        \"\"\"\n",
    "\n",
    "        # for this particular loss, one may also elementwise multiply y_pred and y_true with the inverted mask\n",
    "        masked_pred = torch.masked_select(y_pred, mask)\n",
    "        masked_true = torch.masked_select(y_true, mask)\n",
    "\n",
    "        return self.mse_loss(masked_pred, masked_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Any\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules import MultiheadAttention, Linear, Dropout, BatchNorm1d, TransformerEncoderLayer\n",
    "\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    raise ValueError(\n",
    "        \"activation should be relu/gelu, not {}\".format(activation))\n",
    "\n",
    "\n",
    "# From https://github.com/pytorch/examples/blob/master/word_language_model/model.py\n",
    "class FixedPositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens\n",
    "        in the sequence. The positional encodings have the same dimension as\n",
    "        the embeddings, so that the two can be summed. Here, we use sine and cosine\n",
    "        functions of different frequencies.\n",
    "    .. math::\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=1024).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=1024, scale_factor=1.0):\n",
    "        super(FixedPositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)  # positional encoding\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() *\n",
    "            (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = scale_factor * pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\n",
    "            'pe', pe\n",
    "        )  # this stores the variable in the state_dict (used for non-trainable variables)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class LearnablePositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=1024):\n",
    "        super(LearnablePositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # Each position gets its own embedding\n",
    "        # Since indices are always 0 ... max_len, we don't have to do a look-up\n",
    "        self.pe = nn.Parameter(torch.empty(\n",
    "            max_len, 1, d_model))  # requires_grad automatically set to True\n",
    "        nn.init.uniform_(self.pe, -0.02, 0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "def get_pos_encoder(pos_encoding):\n",
    "    if pos_encoding == \"learnable\":\n",
    "        return LearnablePositionalEncoding\n",
    "    elif pos_encoding == \"fixed\":\n",
    "        return FixedPositionalEncoding\n",
    "\n",
    "    raise NotImplementedError(\n",
    "        \"pos_encoding should be 'learnable'/'fixed', not '{}'\".format(\n",
    "            pos_encoding))\n",
    "\n",
    "\n",
    "class TransformerBatchNormEncoderLayer(nn.modules.Module):\n",
    "    r\"\"\"This transformer encoder layer block is made up of self-attn and feedforward network.\n",
    "    It differs from TransformerEncoderLayer in torch/nn/modules/transformer.py in that it replaces LayerNorm\n",
    "    with BatchNorm.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 nhead,\n",
    "                 dim_feedforward=2048,\n",
    "                 dropout=0.1,\n",
    "                 activation=\"relu\"):\n",
    "        super(TransformerBatchNormEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = BatchNorm1d(\n",
    "            d_model, eps=1e-5\n",
    "        )  # normalizes each feature across batch samples and time steps\n",
    "        self.norm2 = BatchNorm1d(d_model, eps=1e-5)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(TransformerBatchNormEncoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                src_mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        src2 = self.self_attn(src,\n",
    "                              src,\n",
    "                              src,\n",
    "                              attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)  # (seq_len, batch_size, d_model)\n",
    "        src = src.permute(1, 2, 0)  # (batch_size, d_model, seq_len)\n",
    "        # src = src.reshape([src.shape[0], -1])  # (batch_size, seq_length * d_model)\n",
    "        src = self.norm1(src)\n",
    "        src = src.permute(2, 0, 1)  # restore (seq_len, batch_size, d_model)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)  # (seq_len, batch_size, d_model)\n",
    "        src = src.permute(1, 2, 0)  # (batch_size, d_model, seq_len)\n",
    "        src = self.norm2(src)\n",
    "        src = src.permute(2, 0, 1)  # restore (seq_len, batch_size, d_model)\n",
    "        return src\n",
    "\n",
    "\n",
    "class TSTransformerEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 feat_dim,\n",
    "                 max_len,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 num_layers,\n",
    "                 dim_feedforward,\n",
    "                 dropout=0.1,\n",
    "                 pos_encoding='fixed',\n",
    "                 activation='gelu',\n",
    "                 norm='BatchNorm',\n",
    "                 freeze=False):\n",
    "        super(TSTransformerEncoder, self).__init__()\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.project_inp = nn.Linear(feat_dim, d_model)\n",
    "        self.pos_enc = get_pos_encoder(pos_encoding)(d_model,\n",
    "                                                     dropout=dropout *\n",
    "                                                     (1.0 - freeze),\n",
    "                                                     max_len=max_len)\n",
    "\n",
    "        if norm == 'LayerNorm':\n",
    "            encoder_layer = TransformerEncoderLayer(d_model,\n",
    "                                                    self.n_heads,\n",
    "                                                    dim_feedforward,\n",
    "                                                    dropout * (1.0 - freeze),\n",
    "                                                    activation=activation)\n",
    "        else:\n",
    "            encoder_layer = TransformerBatchNormEncoderLayer(\n",
    "                d_model,\n",
    "                self.n_heads,\n",
    "                dim_feedforward,\n",
    "                dropout * (1.0 - freeze),\n",
    "                activation=activation)\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, num_layers)\n",
    "\n",
    "        self.output_layer = nn.Linear(d_model, feat_dim)\n",
    "\n",
    "        self.act = _get_activation_fn(activation)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.feat_dim = feat_dim\n",
    "\n",
    "    def forward(self, X, padding_masks):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: (batch_size, seq_length, feat_dim) torch tensor of masked features (input)\n",
    "            padding_masks: (batch_size, seq_length) boolean tensor, 1 means keep vector at this position, 0 means padding\n",
    "        Returns:\n",
    "            output: (batch_size, seq_length, feat_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        # permute because pytorch convention for transformers is [seq_length, batch_size, feat_dim]. padding_masks [batch_size, feat_dim]\n",
    "        inp = X.permute(1, 0, 2)\n",
    "        inp = self.project_inp(inp) * math.sqrt(\n",
    "            self.d_model\n",
    "        )  # [seq_length, batch_size, d_model] project input vectors to d_model dimensional space\n",
    "        inp = self.pos_enc(inp)  # add positional encoding\n",
    "        # NOTE: logic for padding masks is reversed to comply with definition in MultiHeadAttention, TransformerEncoderLayer\n",
    "        output = self.transformer_encoder(\n",
    "            inp, src_key_padding_mask=~padding_masks\n",
    "        )  # (seq_length, batch_size, d_model)\n",
    "        output = self.act(\n",
    "            output\n",
    "        )  # the output transformer encoder/decoder embeddings don't include non-linearity\n",
    "        output = output.permute(1, 0, 2)  # (batch_size, seq_length, d_model)\n",
    "        output = self.dropout1(output)\n",
    "        # Most probably defining a Linear(d_model,feat_dim) vectorizes the operation over (seq_length, batch_size).\n",
    "        output = self.output_layer(\n",
    "            output)  # (batch_size, seq_length, feat_dim)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSTLightning(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            fold=None,\n",
    "            training_set=None,\n",
    "            in_features=None,\n",
    "            out_features=breath_steps,\n",
    "            d_model=128,  # total dimension of the model (number of features created by the model) Usual values: 128-1024.\n",
    "            n_heads=8,  # parallel attention heads. Usual values: 8-16.\n",
    "            num_layers=3,  # the number of sub-encoder-layers in the encoder. Usual values: 2-8.\n",
    "            dim_feedforward=256,  # the dimension of the feedforward network model. Usual values: 256-4096.\n",
    "            dropout=0.1,  # amount of residual dropout applied in the encoder. Usual values: 0.-0.3.\n",
    "            pos_encoding='fixed',  # fixed, learnable\n",
    "            activation='gelu',  # # activation function of intermediate layer, relu or gelu.\n",
    "            norm='BatchNorm',  # BatchNorm, LayerNorm\n",
    "            learning_rate=1e-3):\n",
    "        super(TSTLightning, self).__init__()\n",
    "\n",
    "        self.fold = fold\n",
    "        self.training_set = training_set\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.model = TSTransformerEncoder(\n",
    "            feat_dim=in_features,\n",
    "            max_len=breath_steps,\n",
    "            d_model=d_model,\n",
    "            n_heads=n_heads,\n",
    "            num_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            pos_encoding=pos_encoding,  # fixed, learnable\n",
    "            activation=activation,  # relu, gelu\n",
    "            norm=norm,  # BatchNorm, LayerNorm\n",
    "            freeze=False)\n",
    "\n",
    "        self.num_parameters = count_parameters(self.model)\n",
    "        print(f\"Trainable params: {self.num_parameters:,}\")\n",
    "\n",
    "        self.loss_fn = MaskedMSELoss(reduction=\"mean\")\n",
    "\n",
    "        # Save passed hyperparameters\n",
    "        self.save_hyperparameters(\"in_features\", \"d_model\", \"n_heads\",\n",
    "                                  \"num_layers\", \"dim_feedforward\", \"dropout\",\n",
    "                                  \"pos_encoding\", \"activation\", \"norm\",\n",
    "                                  \"learning_rate\")\n",
    "\n",
    "        # Important: Activates manual optimization\n",
    "        # https://pytorch-lightning.readthedocs.io/en/stable/common/optimizers.html#manual-optimization\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "    def forward(self, x, masks):\n",
    "        # print(x.shape, masks.shape)\n",
    "        return self.model(x, masks)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        opt = self.optimizers()\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        X, targets, target_masks, padding_masks = batch\n",
    "\n",
    "        logits = self(X, padding_masks)  # (batch_size, breath_steps)\n",
    "\n",
    "        # Cascade noise masks (batch_size, padded_length, feat_dim) and padding masks (batch_size, padded_length)\n",
    "        target_masks = target_masks * padding_masks.unsqueeze(-1)\n",
    "\n",
    "        loss = self.loss_fn(\n",
    "            logits, targets, target_masks\n",
    "        )  # (num_active,) individual loss (square error per element) for each active value in batch\n",
    "\n",
    "        current_lr = self.lr_schedulers().get_last_lr()[0]\n",
    "\n",
    "        self.manual_backward(loss)\n",
    "\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(self.parameters(),\n",
    "                                                   gradient_clip_val)\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "        scheduler = self.lr_schedulers()\n",
    "        scheduler.step()\n",
    "\n",
    "        self.log_dict({\n",
    "            'train_loss': loss,\n",
    "            'learning_rate': current_lr\n",
    "        },\n",
    "                      on_step=True,\n",
    "                      on_epoch=True,\n",
    "                      prog_bar=True,\n",
    "                      logger=True)\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "        pass\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n",
    "        # Extract z representation features\n",
    "        with torch.no_grad():\n",
    "            X, u_out = batch\n",
    "\n",
    "            batch_size = X.shape[0]\n",
    "            padding_masks = torch.zeros(\n",
    "                batch_size, breath_steps, dtype=torch.bool,\n",
    "                device=self.device)  # (batch_size, padded_length)\n",
    "            for i in range(batch_size):\n",
    "                padding_masks[i, :] = torch.where(u_out[i] == 0, 1, 0)\n",
    "\n",
    "            logits = self(X.float(),\n",
    "                          padding_masks)  # (batch_size, breath_steps)\n",
    "            return logits.detach().cpu()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if self.training:\n",
    "            self.train_dataset = VPPMaskedInputDataset(self.training_set[0],\n",
    "                                                       self.training_set[1])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataloader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=train_batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=lambda x: collate_unsuperv(x, max_len=breath_steps),\n",
    "            drop_last=False)\n",
    "        print(f\"Train iterations: {len(train_dataloader)}\")\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        pass\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        pass\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        print(f\"Initial Learning Rate: {self.hparams.learning_rate:.6f}\")\n",
    "\n",
    "        adam_beta1 = 0.9\n",
    "        adam_beta2 = 0.999\n",
    "        adam_epsilon = 1e-8\n",
    "        optimizer = RAdam(self.parameters(),\n",
    "                          lr=self.hparams.learning_rate,\n",
    "                          betas=(adam_beta1, adam_beta2),\n",
    "                          eps=adam_epsilon,\n",
    "                          weight_decay=weight_decay,\n",
    "                          degenerated_to_sgd=True)\n",
    "\n",
    "        train_steps = epochs * (len(self.train_dataloader()) //\n",
    "                                accumulate_grad_batches)\n",
    "        print(f\"Total number of training steps: {train_steps}\")\n",
    "\n",
    "        scheduler = lr_scheduler.MultiStepLR(\n",
    "            optimizer,\n",
    "            milestones=list(range(lr_decay_steps, train_steps,\n",
    "                                  lr_decay_steps)),\n",
    "            gamma=lr_decay_rate)\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(fold_i,\n",
    "              training_set,\n",
    "              in_features=None,\n",
    "              model_path=None,\n",
    "              print_model=False):\n",
    "\n",
    "    if training_mode:\n",
    "        model = TSTLightning(\n",
    "            fold=fold_i,\n",
    "            training_set=training_set,\n",
    "            in_features=in_features,\n",
    "            out_features=breath_steps,\n",
    "            d_model=d_model,\n",
    "            n_heads=n_heads,\n",
    "            num_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            pos_encoding=pos_encoding,  # fixed, learnable\n",
    "            activation=activation,  # relu, gelu\n",
    "            norm=norm,  # BatchNorm, LayerNorm\n",
    "            learning_rate=learning_rate)\n",
    "        if print_model:\n",
    "            print(model)\n",
    "    else:\n",
    "        model = TSTLightning.load_from_checkpoint(\n",
    "            model_path,\n",
    "            fold=fold_i,\n",
    "            training_set=training_set,\n",
    "            in_features=in_features,\n",
    "        )\n",
    "\n",
    "        model.freeze()\n",
    "        model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1120\n"
     ]
    }
   ],
   "source": [
    "# Ensure Reproducibility\n",
    "seed_everything(rand_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 13:23:56.727620: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupervised Train Shape: (125750, 80, 9), (125750, 80)\n",
      "Trainable params: 410,121\n",
      "Initial Learning Rate: 0.001000\n",
      "Train iterations: 983\n",
      "Total number of training steps: 983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type                 | Params\n",
      "-------------------------------------------------\n",
      "0 | model   | TSTransformerEncoder | 410 K \n",
      "1 | loss_fn | MaskedMSELoss        | 0     \n",
      "-------------------------------------------------\n",
      "410 K     Trainable params\n",
      "0         Non-trainable params\n",
      "410 K     Total params\n",
      "1.640     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train iterations: 983\n",
      "Epoch 0: 100%|| 983/983 [00:18<00:00, 52.63it/s, v_num=0, train_loss_step=0.102, learning_rate_step=0.001] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 982: train_loss_epoch reached 0.18977 (best 0.18977), saving model to \"/home/iai/Desktop/bongjun/PyTorch_practice/lightning-unsupervised-tst/epoch=0-train_loss_epoch=0.189767.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|| 983/983 [00:18<00:00, 52.33it/s, v_num=0, train_loss_step=0.102, learning_rate_step=0.001]\n"
     ]
    }
   ],
   "source": [
    "if training_mode:\n",
    "    print(f\"Unsupervised Train Shape: {all_features.shape}, {all_u_out.shape}\")\n",
    "\n",
    "    logger = TensorBoardLogger(model_output_folder,\n",
    "                               name=f\"logs\",\n",
    "                               default_hp_metric=True)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=f\"{model_output_folder}\",\n",
    "        filename=\"{epoch}-{train_loss_epoch:.6f}\",\n",
    "        save_top_k=1,\n",
    "        save_weights_only=True,\n",
    "        save_last=False,\n",
    "        verbose=True,\n",
    "        monitor='train_loss_epoch',\n",
    "        mode='min')\n",
    "\n",
    "    callbacks = [checkpoint_callback]\n",
    "\n",
    "    model = get_model(fold_i=None,\n",
    "                      training_set=(all_features, all_u_out),\n",
    "                      in_features=all_features.shape[-1])\n",
    "\n",
    "    trainer = Trainer(\n",
    "        gpus=gpus if torch.cuda.is_available() else None,\n",
    "        distributed_backend=\"dp\"\n",
    "        if torch.cuda.is_available() else None,  # multiple-gpus, 1 machine\n",
    "        max_epochs=epochs,\n",
    "        benchmark=False,\n",
    "        deterministic=True,\n",
    "        log_gpu_memory=False,\n",
    "        checkpoint_callback=True,\n",
    "        callbacks=callbacks,\n",
    "        accumulate_grad_batches=accumulate_grad_batches,\n",
    "        precision=16 if mixed_precision and torch.cuda.is_available() else 32,\n",
    "        logger=logger)\n",
    "\n",
    "    trainer.fit(model)\n",
    "    \n",
    "    del model, trainer\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 410,121\n"
     ]
    }
   ],
   "source": [
    "training_mode = False\n",
    "pretrained_model_path = '/home/iai/Desktop/bongjun/PyTorch_practice/lightning-unsupervised-tst/epoch=0-train_loss_epoch=0.189767.ckpt'\n",
    "model = get_model(model_path=pretrained_model_path,\n",
    "                  fold_i=None,\n",
    "                  training_set=(None, None),\n",
    "                  in_features=train_features.shape[-1])\n",
    "# Drop output layer\n",
    "model.model.output_layer = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "TSTLightning                                                 [2, 80, 128]              --\n",
       "TSTransformerEncoder: 1-1                                  [2, 80, 128]              --\n",
       "    Linear: 2-1                                           [80, 2, 128]              (1,280)\n",
       "    LearnablePositionalEncoding: 2-2                      [80, 2, 128]              10,240\n",
       "        Dropout: 3-1                                     [80, 2, 128]              --\n",
       "    TransformerEncoder: 2-3                               [80, 2, 128]              --\n",
       "        ModuleList: 3-2                                  --                        (397,440)\n",
       "    Dropout: 2-4                                          [2, 80, 128]              --\n",
       "    Identity: 2-5                                         [2, 80, 128]              --\n",
       "==============================================================================================================\n",
       "Total params: 408,960\n",
       "Trainable params: 0\n",
       "Non-trainable params: 408,960\n",
       "Total mult-adds (M): 15.93\n",
       "==============================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 2.62\n",
       "Params size (MB): 0.80\n",
       "Estimated Total Size (MB): 3.43\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model.cuda(),\n",
    "        input_size=[(2, breath_steps, train_features.shape[-1]),\n",
    "                    (2, breath_steps)],\n",
    "        dtypes=[torch.float, torch.bool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1120\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "# Ensure Reproducibility\n",
    "seed_everything(rand_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "trainer = Trainer(\n",
    "    logger=False,\n",
    "    gpus=gpus if torch.cuda.is_available() else None,\n",
    "    distributed_backend=\"dp\" if torch.cuda.is_available() else None,\n",
    "    precision=16 if mixed_precision and torch.cuda.is_available() else 32,\n",
    "    benchmark=False,\n",
    "    deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|| 37/37 [00:05<00:00,  6.81it/s]\n",
      "torch.Size([75450, 80, 128])\n",
      "tensor([[ 1.2916e-01,  1.7509e+00, -2.5348e-02,  ..., -1.1493e-01,\n",
      "          1.8674e-01, -1.5192e-01],\n",
      "        [ 2.0865e-01,  1.2915e+00,  1.8705e-01,  ..., -1.3614e-01,\n",
      "          1.4410e-01, -1.6098e-01],\n",
      "        [ 2.2961e-01,  1.1692e+00,  2.4726e-01,  ..., -1.3807e-01,\n",
      "          1.2992e-01, -1.6391e-01],\n",
      "        ...,\n",
      "        [ 5.2102e-01,  5.6759e-01, -1.9444e-02,  ...,  2.2709e+00,\n",
      "          7.4782e-01,  2.0103e-03],\n",
      "        [ 5.2374e-01,  5.4254e-01, -1.6139e-02,  ...,  2.3304e+00,\n",
      "          7.4883e-01,  1.7196e-02],\n",
      "        [ 5.2781e-01,  5.2074e-01, -1.3628e-02,  ...,  2.3826e+00,\n",
      "          7.4843e-01,  3.0846e-02]])\n"
     ]
    }
   ],
   "source": [
    "pred_dataset = VPPTestDataset(train_features, train_u_out)\n",
    "pred_dataloader = torch.utils.data.DataLoader(pred_dataset,\n",
    "                                              batch_size=infer_batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=num_workers,\n",
    "                                              pin_memory=True,\n",
    "                                              drop_last=False)\n",
    "\n",
    "pred_logits = trainer.predict(model,\n",
    "                              dataloaders=pred_dataloader,\n",
    "                              return_predictions=True)\n",
    "pred_logits = torch.cat(pred_logits, dim=0)\n",
    "train_encoder_features = pred_logits.numpy()\n",
    "print(pred_logits.shape)\n",
    "print(pred_logits[0, :, :])\n",
    "\n",
    "filename = f\"train_encoder_{d_model}features.pkl\"\n",
    "save_pickle(train_encoder_features, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1380"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del pred_logits, train_encoder_features\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|| 25/25 [00:03<00:00,  6.69it/s]\n",
      "torch.Size([50300, 80, 128])\n",
      "tensor([[-0.0402,  1.1135, -0.0901,  ..., -0.0643, -0.0876,  0.9859],\n",
      "        [-0.0486,  0.8995, -0.0370,  ..., -0.0618, -0.0929,  0.9747],\n",
      "        [-0.0605,  0.6213,  0.0393,  ..., -0.0617, -0.0962,  0.9667],\n",
      "        ...,\n",
      "        [-0.0391, -0.0957, -0.0656,  ...,  1.0870, -0.0625,  2.1060],\n",
      "        [-0.0394, -0.0923, -0.0580,  ...,  1.1468, -0.0623,  2.1386],\n",
      "        [-0.0397, -0.0892, -0.0516,  ...,  1.2016, -0.0621,  2.1666]])\n"
     ]
    }
   ],
   "source": [
    "pred_dataset = VPPTestDataset(test_features, test_u_out)\n",
    "pred_dataloader = torch.utils.data.DataLoader(pred_dataset,\n",
    "                                              batch_size=infer_batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=num_workers,\n",
    "                                              pin_memory=True,\n",
    "                                              drop_last=False)\n",
    "\n",
    "pred_logits = trainer.predict(model,\n",
    "                              dataloaders=pred_dataloader,\n",
    "                              return_predictions=True)\n",
    "pred_logits = torch.cat(pred_logits, dim=0)\n",
    "test_encoder_features = pred_logits.numpy()\n",
    "print(pred_logits.shape)\n",
    "print(pred_logits[0, :, :])\n",
    "\n",
    "filename = f\"test_encoder_{d_model}features.pkl\"\n",
    "save_pickle(test_encoder_features, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del pred_logits, test_encoder_features\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bongjun_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
