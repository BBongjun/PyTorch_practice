{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer 구조 구현\n",
    "### Reference\n",
    "<Reference> https://kaya-dev.tistory.com/8 </br>\n",
    "<Reference> https://kaya-dev.tistory.com/11 </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 구현\n",
    "- PositionalEncoding\n",
    "- Self-Attention\n",
    "- Multi-head Attention\n",
    "- Layer Norm layer\n",
    "- Feed-Foward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, max_len, d_model, device):\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "\n",
    "        self.encoding = torch.zeros(max_len, d_model, device =device)\n",
    "        self.encoding.requires_grad = False #그래디언트 계산할 필요 없음\n",
    "\n",
    "        pos = torch.arange(0,max_len,device=device)\n",
    "        pos = pos.float().unsqueeze(dim=1)\n",
    "\n",
    "        _2i = torch.arange(0, d_model, step=2, device = device).float()\n",
    "\n",
    "        self.encoding[:,0::2] = torch.sin(pos/(10000**(_2i/d_model)))\n",
    "        self.encoding[:,1::2] = torch.cos(pos/(10000**(_2i/d_model)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size , seq_len = x.size()\n",
    "\n",
    "        return self.encoding[:,seq_len,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(nn.Module):\n",
    "    # attention score를 계산하는 class\n",
    "    # Query : focus할 sentence\n",
    "    # Key : to check relationship with Query\n",
    "    # Value : every sentence same with key\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ScaleDotProductAttention,self).__init__()\n",
    "\n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, q, k, v, mask = None, e = 1e-12):\n",
    "        # [batch_size, head, length, d_tensor]\n",
    "        batch_size, head, length, d_tensor = k.size()\n",
    "\n",
    "        # Step 1 : Q x K^T 를 통해, 유사도 계산(dop product)\n",
    "        k_t = k.view(batch_size, head, d_tensor, length)\n",
    "        score = (q @ k_t) / math.sqrt(d_tensor) # @ 연산은 np.matmul 과 같은 역할이라고 함\n",
    "\n",
    "        # Step 2 : applying masking(optional)\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask==0, -e) \n",
    "\n",
    "        # Step 3 : pass tem softmax to make [0,1] range\n",
    "        score = self.softmax(score)\n",
    "\n",
    "        # Step 4 : Multiply with Value\n",
    "        v = score @ v\n",
    "\n",
    "        return v, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.attention = ScaleDotProductAttention()\n",
    "        self.w_q = nn.Linear(d_model,d_model)\n",
    "        self.w_k = nn.Linear(d_model,d_model)\n",
    "        self.w_v = nn.Linear(d_model,d_model)\n",
    "        self.w_concat = nn.Linear(d_model,d_model)\n",
    "\n",
    "    def split(self,tensor):\n",
    "        \"\"\"\n",
    "        split tensor by number of head\n",
    "\n",
    "        param tensor = [batch_size, length, d_model]\n",
    "        out = [batch_size, head, length, d_tensor]\n",
    "\n",
    "        예시)\n",
    "        d_model = 512 일 때, head를 8개 쓰고 싶다?\n",
    "        d_tensor = 512/8 = 64\n",
    "        \"\"\"\n",
    "        batch_size, length, d_model = tensor.size() # [B, L, d_model]\n",
    "\n",
    "        d_tensor = d_model//self.n_head # 64\n",
    "\n",
    "        tensor = tensor.view(batch_size,self.n_head,length,d_tensor) # [B, H, L, d_tensor]\n",
    "\n",
    "        return tensor\n",
    "    \n",
    "    def concat(self, tensor):\n",
    "        \"\"\"\n",
    "        inverse function of self.split(tensor = torch.Tensor)\n",
    "\n",
    "        param tensor = [batch_size, head, length, d_tensor]\n",
    "        out = [batch_size, length, d_model]\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, head, length, d_tensor = tensor.size()\n",
    "        d_model = d_tensor * head\n",
    "\n",
    "        tensor = tensor.view(batch_size, length, d_model)\n",
    "        return tensor\n",
    "    \n",
    "    def forward(self, q,k,v,mask = None):\n",
    "\n",
    "        # Step 1 : dot product with weight metrics\n",
    "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
    "\n",
    "        # Step 2 : split tensor by number of heads\n",
    "        q, k, v = self.split(q), self.split(k), self.split(v)\n",
    "\n",
    "        # Step 3 : ScaleDotProductionAttention 으로 Attention vector 및 Attention score 계산\n",
    "        out , attention = self.attention(q, k, v, mask = mask)\n",
    "\n",
    "        # Step 4 : concat and pass to linear layer\n",
    "        out = self.concat(out)\n",
    "        out = self.w_concat(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,d_model,eps = 1e-12):\n",
    "        super(LayerNorm,self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model)) # scaling parameter\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model)) # 평균값을 조정할 수 있는 parameter\n",
    "        self.eps = eps # 정규화시에 분모가 0이 되는 경우를 방지해주는 parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        # -1 means last dimension = 마지막 차원에 대해서 평균, 분산을 구해야 함\n",
    "        # keepdim을 통해, 차원을 유지해야지, 밑에 코드로 각 차원에 대한 평균, 분산으로 정규화 진행할 수 있음\n",
    "\n",
    "        out = (x-mean)/(std + eps)\n",
    "        out = self.gamma * out + self.beta\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeadFoward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeadFoward,self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model,hidden)\n",
    "        self.linear2 = nn.Linear(hidden,d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p = drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder 및 Decoder 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,d_model, ffn_hidden, n_head, drop_prob):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "\n",
    "        # Multi-Head Attention\n",
    "        self.attention = MultiHeadAttention(d_model, n_head)\n",
    "\n",
    "        # Layer Norm layer(After MHA)\n",
    "        self.norm1 = LayerNorm(d_model = d_model)\n",
    "        self.dropout1 = nn.Dropout(p = drop_prob)\n",
    "\n",
    "        # Feed-Forward\n",
    "        self.ffn = PositionwiseFeadFoward(d_model, ffn_hidden, drop_prob)\n",
    "\n",
    "        # Layer Norm layer(After FFN)\n",
    "        self.norm2 = LayerNorm(d_model = d_model)\n",
    "        self.dropout2 = nn.Dropout(p = drop_prob)\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        _x = x\n",
    "\n",
    "        # Step 1 : Compute Multi-Head Attention\n",
    "        x = self.attention(q = x, k = x, v = x, mask = src_mask)\n",
    "\n",
    "        # Step 2 : Compute Add & Layer Norm\n",
    "        x = self.norm1(x+_x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Step 3 : Compute Feed-Forward Network\n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "\n",
    "        # Step 4 : Compute Add & Layer Norm\n",
    "        x = self.norm2(x+_x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, enc_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding\n",
    "        self.embed = nn.Embedding(num_embeddings= enc_voc_size, embedding_dim= d_model, padding_idx = 1)\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.pe = PositionalEncoding(max_len = max_len, d_model = d_model, device = device)\n",
    "\n",
    "        # Add Multi layer\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model = d_model,\n",
    "                                                  ffn_hidden = ffn_hidden,\n",
    "                                                  n_head = n_head,\n",
    "                                                  drop_prob = drop_prob)\n",
    "                                                  for _ in range(n_layers)])\n",
    "        \n",
    "    def forward(self, x, src_mask):\n",
    "        # Step 1 : Compute Embedding\n",
    "        x = self.embed(x)  # sentence => vector\n",
    "\n",
    "        # Step 2 : Get Positional Encoding\n",
    "        x_pe = self.pe(x)\n",
    "\n",
    "        # Step 3 : Embedding + Positional Encoding\n",
    "        x = x + x_pe\n",
    "\n",
    "        # Step 4 : Compute Encoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 첫번째 MHA는 Encoder와 동일.\n",
    "- encoder의 결과 값이 Decoder의 두번째 MHA에서 Key, Value로 들어감.\n",
    "- 나머지 Add & Norm / FFN 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model = d_model, n_head = n_head)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model = d_model)\n",
    "        self.dropout1 = nn.Dropout(p = drop_prob)\n",
    "\n",
    "        self.enc_dec_attention = MultiHeadAttention(d_model = d_model, n_head = n_head)\n",
    "\n",
    "        self.norm2 = LayerNorm(d_model = d_model)\n",
    "        self.dropout2 = nn.Dropout(p = drop_prob)\n",
    "\n",
    "        self.ffn = PositionwiseFeadFoward(d_model=d_model, hidden = ffn_hidden, drop_prob = drop_prob)\n",
    "\n",
    "        self.norm3 = LayerNorm(d_model = d_model)\n",
    "        self.dropout3 = nn.Dropout(p = drop_prob)\n",
    "\n",
    "    def forward(self, dec, enc, trg_mask, src_mask): #여기서 trg_mask와 src_mask는 뭐지?\n",
    "        _x = dec\n",
    "\n",
    "        # Step 1 : Compute self-attention\n",
    "        x = self.self_attention(q = dec, k = dec, v = dec, mask = trg_mask)\n",
    "        \n",
    "        # Step 2 : Add & Norm\n",
    "        x = self.norm1(x+_x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Step 3 : Compute enc_dec_attention\n",
    "        if enc is not None:\n",
    "            _x = x\n",
    "\n",
    "            # 여기서 기존의 self-attention과는 다르게 동작\n",
    "            # Query : Decoder attention output\n",
    "            # Key : Encoder output\n",
    "            # Value : Encoder output\n",
    "\n",
    "            x = self.enc_dec_attention(q = dec, k = enc, v = enc, mask = src_mask)\n",
    "\n",
    "            # compute add & norm\n",
    "            x = self.norm2(x+_x)\n",
    "            x = self.dropout2(x)\n",
    "\n",
    "        # Step 4 : FFN\n",
    "        _x = x\n",
    "\n",
    "        x = self.ffn(x)\n",
    "\n",
    "        # Step 5 : Add & Norm\n",
    "        x = self.norm3(x+_x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dec_voc_size, max_len, d_model, ffn_hidden, n_head, n_layer, drop_prob, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(num_embeddings= dec_voc_size, embedding_dim=d_model, padding_idx = 1)\n",
    "\n",
    "        self.pe = PositionalEncoding(max_len = max_len, d_model = d_model, device = device)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model = d_model,\n",
    "                                                  ffn_hidden = ffn_hidden,\n",
    "                                                  n_head = n_head,\n",
    "                                                  drop_prob = drop_prob)\n",
    "                                                  for _ in range(n_layer)])\n",
    "                                                  \n",
    "        #Linear\n",
    "        self.linear = nn.Linear(d_model, dec_voc_size)\n",
    "\n",
    "    def forward(self,trg,src,trg_mask,src_mask):\n",
    "\n",
    "        # Step 1 : Compute Embedding\n",
    "        trg = self.embed(trg)\n",
    "\n",
    "        # Step 2 : Get Positional Encoding\n",
    "        trg_pe = self.pe(trg)\n",
    "\n",
    "        # Step 3 : Embedding + PE\n",
    "        trg = trg + trg_pe\n",
    "\n",
    "        # Step 4 : Compute Decoder layers\n",
    "        for layer in self.layers:\n",
    "            trg = layer(dec = trg, enc = src, trg_mask = trg_mask, src_mask = src_mask)\n",
    "        \n",
    "        #pass to LM head\n",
    "        \n",
    "        output = self.linear(trg)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer 구현(이해 더 필요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self,src_pad_idx,trg_pad_idx,trg_sos_idx,enc_voc_size,dec_voc_size,d_model,n_head,max_len,\n",
    "                ffn_hidden,n_layers,drop_prob,device):\n",
    "        super().__init__()\n",
    "        #Get <PAD> idx\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.trg_sos_idx = trg_sos_idx\n",
    "        \n",
    "        #Encoder\n",
    "        self.encoder = Encoder(enc_voc_size = enc_voc_size,\n",
    "                              max_len = max_len,\n",
    "                              d_model = d_model,\n",
    "                              ffn_hidden = ffn_hidden,\n",
    "                              n_head = n_head,\n",
    "                              n_layers = n_layers,\n",
    "                              drop_prob = drop_prob,\n",
    "                              device = device)\n",
    "        \n",
    "        #Decoder\n",
    "        self.decoder = Decoder(dec_voc_size = dec_voc_size,\n",
    "                              max_len = max_len,\n",
    "                              d_model = d_model,\n",
    "                              ffn_hidden = ffn_hidden,\n",
    "                              n_head = n_head,\n",
    "                              n_layers = n_layers,\n",
    "                              drop_prob = drop_prob,\n",
    "                              device = device)\n",
    "        self.device = device\n",
    "    \n",
    "    def make_pad_mask(self,q,k):\n",
    "    \n",
    "    \t#Padding부분은 attention연산에서 제외해야하므로 mask를 씌워줘서 계산이 되지 않도록 한다.\n",
    "        \n",
    "        len_q,len_k = q.size(1),k.size(1)\n",
    "        print(len_k)\n",
    "        #batch_size x 1 x 1 x len_k\n",
    "        k = k.ne(self.src_pad_idx).unsqueeze(1).unsqueeze(2) # 패딩 토큰 위치는 False, 나머지는 True인 boolean 텐서를 반환. 여기서 ne는 not equal을 의미\n",
    "        print(k.shape)\n",
    "        # batch_size x 1 x len_q x len_k\n",
    "        k = k.repeat(1,1,len_q,1)\n",
    "        \n",
    "        # batch_size x 1 x len_q x 1\n",
    "        q = q.ne(self.src_pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "        # batch_size x 1 x len_q x len_k\n",
    "        q = q.repeat(1,1,1,len_k)\n",
    "        \n",
    "        mask = k & q  # 패딩 위치 = False, 패딩 위치가 아닌 곳 : True\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def make_no_peak_mask(self,q,k):\n",
    "    \t\n",
    "        #Decoder 부분에서 t번째 단어를 예측하기 위해 입력으로 t-1번째 단어까지 넣어야 하므로 나머지 부분을 masking처리 한다.\n",
    "        #만약 t번째 단어를 예측하는데 이미 decoder에 t번째 단어가 들어간다면?? => 답을 이미 알고 있는 상황..\n",
    "        #따라서 Seq2Seq 모델에서 처럼 t번째 단어를 예측하기 위해서 t-1번째 단어까지만 입력될 필요가 있음\n",
    "        #(나머지 t,t+1,...,max_len)까지 단어는 t번째 단어를 예측하는데 전혀 필요하지 않음 => Masking!!\n",
    "        len_q,len_k = q.size(1),k.size(1)\n",
    "        \n",
    "        #len_q x len_k (torch.tril = 하삼각행렬)\n",
    "        mask = torch.tril(torch.ones(len_q,len_k)).type(torch.BoolTensor).to(self.device) \n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def forward(self,src,trg):\n",
    "    \t\n",
    "        #Get Mask\n",
    "        src_mask = self.make_pad_mask(src,src)\n",
    "        src_trg_mask = self.make_pad_mask(trg,src)\n",
    "        trg_mask = self.make_pad_mask(trg,trg) * self.make_no_peak_mask(trg,trg)\n",
    "        \n",
    "        #Compute Encoder\n",
    "        enc_src = self.encoder(src,src_mask)\n",
    "        \n",
    "        #Compute Decoder\n",
    "        output = self.decoder(trg,enc_src,trg_mask,src_trg_mask)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bongjun_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
